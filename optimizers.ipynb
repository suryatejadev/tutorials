{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "## Batch GD\n",
    "- Uses entire data, not practical.  \n",
    "\n",
    "## SGD\n",
    "- One training example at a time. \n",
    "- Much faster. \n",
    "- Has convergence guarantees. \n",
    "- High variance helps escape local minima.  \n",
    "- Decreasing LR by Annealing helps stabilize convergence.  \n",
    "\n",
    "## Minibatch SGD\n",
    "- Batch SGD\n",
    "- reduces variance of updates, so more stable convergence.\n",
    "\n",
    "## Challenges\n",
    "- Difficult to choose LR, adaptive LR schedules\n",
    "- [saddle points](https://arxiv.org/abs/1406.2572) (points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions).  \n",
    "- Navigating ravines - Common around local optima. SGD has a hard time with this. \n",
    "\n",
    "## Notes: \n",
    "- **Avoiding Saddle points** - start with random weight initializations. \n",
    "![image.png](images/saddle_point.png)\n",
    "- **Avoiding Plateaux** - Use leaky Relu\n",
    "- **Avoiding Ravines**\n",
    "    - Recenter hidden units to zero mean and unit variance, use batchnorm\n",
    "    - An area of research known as second-order optimization develops algorithms which explicitly use curvature information, but these are complicated and difficult to scale to large neural nets and large datasets.\n",
    "    - There is an optimization procedure called Adam which uses just a little bit of curvature information and often works much better than gradient descent. Itâ€™s available in all the major neural net frameworks.  \n",
    "\n",
    "![image.png](images/ravines.png)\n",
    "\n",
    "### Momentum \n",
    "- momentum dampens oscillations due to a ravine. \n",
    "\n",
    "### Adagrad, adadelta, RMSprop\n",
    "- use gradients to calculate adaptive learning rate for each parameter.  \n",
    "\n",
    "### Adam \n",
    "- Can be seen as RMSprop (exponentially decaying average of past squared gradients V_t) and momentum (exponentially decaying average of past gradients m_t).\n",
    "- Calculate estimates for mean and variance for gradients. \n",
    "- Uses that to calculate apative LR for each parameter.  \n",
    "\n",
    "### Adamax\n",
    "- Instead of using L2 norm in v update, use L-infinity norm, as that converges to a more stable value. \n",
    "\n",
    "### Nadam\n",
    "- Combines Nesterov Accelerted GD with Adam. \n",
    "\n",
    "### AMSgrad\n",
    "- Says exponential MA of past squared gradients is the reason for poor generalization of adaptive learning rate methods.  \n",
    "- For v_t, use max of v_t-1 and v_t. \n",
    "- In practice, didn't show much improvement over Adam. \n",
    "\n",
    "### AdamW\n",
    "- It has been shown that L2 regularization will improve Adam. \n",
    "- But the weight regularization should not be added finally to the loss. \n",
    "- It should be added directly to tehe gradient (instead of g_t, use g_t + wt_decay*w). \n",
    "- The wt_decay parameter is also updated as wt_decay = wt_decay(1-lr). \n",
    "- Gives a better performance than Adam. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec7.pdf\n",
    "- http://ruder.io/optimizing-gradient-descent/\n",
    "- https://arxiv.org/abs/1406.2572\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
